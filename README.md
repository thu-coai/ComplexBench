# Benchmarking Complex Instruction-Following with Multiple Constraints Composition

This repository is the official implementation of Benchmarking Complex Instruction-Following with Multiple Constraints Composition. [Our Paper](https://arxiv.org/abs/2407.03978) 

## âš™ï¸ Requirements

To install requirements:

```setup
pip install -r requirements.txt
```

## ğŸ“¦ Data

All data of ComplexBench is placed in `data/data_release.json` . The entire dataset is organized in the format of a list, where each element of the list is an instance of the dataset. The format of an instance is as follows.

- `main_id` (integer): A unique identifier for the instance.
- `group` (string): The task group identifier for The Coherent Test for Selection (Section 5.2.3). Applicable only in instructions that contain Selection.
- `idx_in_group` (integer): Number of the instruction in the task group. Applicable only in instructions that contain Selection.
- `instruction` (string): The actual instruction. 
- `task_types` (string): The task type of the instruction. 
- `constraint_dimensions` (list): All constraint dimensions within the instruction.
- `composition_types` (list): All composition types of constraints within the instruction.
- `category` (list): The category of the instruction based on composition types within it. 
- `scoring_questions` (list): All scoring questions of the instruction, which are designed to verify each constraint dimensions and composition types.  Each element of this list contains:
  - `point_id` (integer): Number of the scoring question.
  - `question` (string): The actual scoring question.
  - `rule` (string): Verification rule for the score scoring. `null` indicates that the score question cannot be verified by rules.
  - `constraint_dimensions` (string): All constraint dimensions that the scoring question verifies.
  - `composition_types` (string): All composition types that the scoring question verifies. 
  - `dep` (list): The numbers of all score questions on which this score question depends.. 

- `sub_instructions` (dict): The decomposed atomic instrcutions based on the instruction (Section 5.2.3). All keys in this dict are in the format of `sub_instruction_x`, where `x` is the number of the decomposed instruction. Each value of this dict contains:
  - `instruction` (string): The actual decomposed atomic instruction. 
  - `scoring_questions` (list): All scoring questions of the decomposed atomic instrcution . 

Here is an example of ComplexBench.

```json
     {
        "main_id": 899,
        "group": "complex_instruction_eval_1285",
        "idx_in_group": 1,
        "instruction": "ä¾æ¬¡åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªæ¡ˆä¾‹ä¸­çš„å›½å®¶æ˜¯å¦æœ‰ç‰¹åˆ«ææ¬¾æƒã€‚å¦‚æœæœ‰ï¼Œè¯·å†™å‡ºä¸€ç¯‡ä¸ºè¯¥å›½ç”³è¯·ææ¬¾çš„æ–‡ç« ï¼Œå­—æ•°ä¸å°‘äº300å­—ï¼Œä¸”åˆ†ç‚¹æ˜ç¡®ã€‚å¦‚æœæ²¡æœ‰åˆ™è§£é‡ŠåŸå› ï¼Œå­—æ•°ä¸è¶…è¿‡100å­—ã€‚\n\næ¡ˆä¾‹1ï¼š\n\nå›½å®¶Aæ˜¯ä¸€ä¸ªå‘å±•ä¸­å›½å®¶ï¼Œæ˜¯å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ï¼ˆIMFï¼‰çš„æˆå‘˜å›½ï¼Œæ­£åœ¨ç»å†ä¸€åœºè‡ªç„¶ç¾å®³ï¼Œå¯¼è‡´è¯¥å›½ç»æµé™·å…¥å±æœºï¼Œå¤±å»äº†å›½é™…æ”¯ä»˜èƒ½åŠ›ã€‚\n\næ¡ˆä¾‹2ï¼š\n\nå›½å®¶Bæ˜¯ä¸€ä¸ªå¯Œè£•å›½å®¶ï¼Œé¢ä¸´ç€å›½å†…é€šè´§è†¨èƒ€é—®é¢˜ï¼ŒBæ˜¯IMFçš„æˆå‘˜å›½ï¼Œæ‹¥æœ‰å……è¶³çš„å¤–æ±‡å‚¨å¤‡ã€‚",
        "task_types": "Professional Writing",
        "constraint_dimensions": [
            "Length",
            "Helpfulness",
            "Bullets Format",
            "Factuality"
        ],
        "composition_types": [
            "And",
            "Selection"
        ],
        "category": "Selection_2",
        "scoring_questions": [
            {
                "point_id": 0,
                "question": "æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆ¤æ–­å›½å®¶Aæœ‰ç‰¹åˆ«ææ¬¾æƒï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Factuality"
                ],
                "composition_types": [
                    "Selection"
                ],
                "dep": []
            },
            {
                "point_id": 1,
                "question": "æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆ¤æ–­å›½å®¶Bæ²¡æœ‰ç‰¹åˆ«ææ¬¾æƒï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Factuality"
                ],
                "composition_types": [
                    "Selection"
                ],
                "dep": []
            },
            {
                "point_id": 2,
                "question": "æ¨¡å‹æ˜¯å¦æ ¹æ®å›½å®¶Aæœ‰ç‰¹åˆ«ææ¬¾æƒç”Ÿæˆç”³è¯·ææ¬¾çš„æ–‡ç« ï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Helpfulness"
                ],
                "composition_types": [],
                "dep": [
                    0
                ]
            },
            {
                "point_id": 3,
                "question": "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦é€»è¾‘åˆç†ï¼Œç¬¦åˆäº‹å®ï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Factuality"
                ],
                "composition_types": [],
                "dep": [
                    0
                ]
            },
            {
                "point_id": 4,
                "question": "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦åœ¨300å­—ä»¥ä¸Šï¼Ÿ",
                "rule": "model_length:[300,10000]",
                "constraint_dimensions": [
                    "Length"
                ],
                "composition_types": [],
                "dep": [
                    0
                ]
            },
            {
                "point_id": 5,
                "question": "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦åˆ†ç‚¹æ˜ç¡®ï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Bullets Format"
                ],
                "composition_types": [],
                "dep": [
                    0
                ]
            },
            {
                "point_id": 6,
                "question": "æ¨¡å‹æ˜¯å¦ç”Ÿæˆå›½å®¶Bæ²¡æœ‰ç‰¹åˆ«ææ¬¾æƒçš„è§£é‡Šï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Helpfulness"
                ],
                "composition_types": [],
                "dep": [
                    1
                ]
            },
            {
                "point_id": 7,
                "question": "æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ˜¯å¦é€»è¾‘åˆç†ï¼Œç¬¦åˆäº‹å®ï¼Ÿ",
                "rule": null,
                "constraint_dimensions": [
                    "Factuality"
                ],
                "composition_types": [],
                "dep": [
                    1
                ]
            },
            {
                "point_id": 8,
                "question": "æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ˜¯å¦ä¸è¶…è¿‡100å­—ï¼Ÿ",
                "rule": "model_length:[1,100]",
                "constraint_dimensions": [
                    "Length"
                ],
                "composition_types": [],
                "dep": [
                    1
                ]
            }
        ],
        "sub_instructions": {
            "sub_instruction_0": {
                "instruction": "åˆ¤æ–­ä»¥ä¸‹ä¸¤ä¸ªæ¡ˆä¾‹ä¸­çš„å›½å®¶æ˜¯å¦æœ‰ç‰¹åˆ«ææ¬¾æƒã€‚\n\næ¡ˆä¾‹1ï¼š\n\nå›½å®¶Aæ˜¯ä¸€ä¸ªå‘å±•ä¸­å›½å®¶ï¼Œæ˜¯å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ï¼ˆIMFï¼‰çš„æˆå‘˜å›½ï¼Œæ­£åœ¨ç»å†ä¸€åœºè‡ªç„¶ç¾å®³ï¼Œå¯¼è‡´è¯¥å›½ç»æµé™·å…¥å±æœºï¼Œå¤±å»äº†å›½é™…æ”¯ä»˜èƒ½åŠ›ã€‚\n\næ¡ˆä¾‹2ï¼š\n\nå›½å®¶Bæ˜¯ä¸€ä¸ªå¯Œè£•å›½å®¶ï¼Œé¢ä¸´ç€å›½å†…é€šè´§è†¨èƒ€é—®é¢˜ï¼ŒBæ˜¯IMFçš„æˆå‘˜å›½ï¼Œæ‹¥æœ‰å……è¶³çš„å¤–æ±‡å‚¨å¤‡ã€‚",
                "scoring_questions": [
                    "æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆ¤æ–­å›½å®¶Aæœ‰ç‰¹åˆ«ææ¬¾æƒï¼Ÿ",
                    "æ¨¡å‹æ˜¯å¦æ­£ç¡®åˆ¤æ–­å›½å®¶Bæ²¡æœ‰ç‰¹åˆ«ææ¬¾æƒï¼Ÿ"
                ]
            },
            "sub_instruction_1": {
                "instruction": "è¯·æ ¹æ®ä¸Šé¢çš„åˆ¤æ–­ï¼Œå®Œæˆä¸‹é¢çš„æŒ‡ä»¤ã€‚\n\n- å¦‚æœæœ‰ï¼Œè¯·å†™å‡ºä¸€ç¯‡ä¸ºè¯¥å›½ç”³è¯·ææ¬¾çš„æ–‡ç« ï¼Œå­—æ•°ä¸å°‘äº300å­—ï¼Œä¸”åˆ†ç‚¹æ˜ç¡®ã€‚\n- å¦‚æœæ²¡æœ‰åˆ™è§£é‡ŠåŸå› ï¼Œå­—æ•°ä¸è¶…è¿‡100å­—ã€‚",
                "scoring_questions": [
                    "æ¨¡å‹æ˜¯å¦æ ¹æ®å›½å®¶Aæœ‰ç‰¹åˆ«ææ¬¾æƒç”Ÿæˆç”³è¯·ææ¬¾çš„æ–‡ç« ï¼Ÿ",
                    "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦é€»è¾‘åˆç†ï¼Œç¬¦åˆäº‹å®ï¼Ÿ",
                    "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦åœ¨300å­—ä»¥ä¸Šï¼Ÿ",
                    "æ¨¡å‹ç”Ÿæˆçš„ç”³è¯·ææ¬¾æ–‡ç« æ˜¯å¦åˆ†ç‚¹æ˜ç¡®ï¼Ÿ",
                    "æ¨¡å‹æ˜¯å¦ç”Ÿæˆå›½å®¶Bæ²¡æœ‰ç‰¹åˆ«ææ¬¾æƒçš„è§£é‡Šï¼Ÿ",
                    "æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ˜¯å¦é€»è¾‘åˆç†ï¼Œç¬¦åˆäº‹å®ï¼Ÿ",
                    "æ¨¡å‹ç”Ÿæˆçš„è§£é‡Šæ˜¯å¦ä¸è¶…è¿‡100å­—ï¼Ÿ"
                ]
            }
        }
    }
```

## ğŸš€ Evaluation

### Step1: Generating the responses

First, you need to deploy your target LLM and generate responses of it (This part is not included in this repository). We have placed the generated responses of 15 LLMs in our paper in `llm_generations`. You can refer the format of one of these files and replace `genereated` field with your target LLM responses.

We suggest using greedy decoding to avoid the randomness of decoding.

### Step2: Evaluation

Then, you can evaluate any desired model via scirpt `eval.sh`:

```bash
bash eval.sh "<path_to_data_release.json>" "<path_to_generated_responsed_in_step_1>" "<path_to_results>" "<openai_API_KEY>" "<openai_API_KEY>" "<your_model_name>"
```

Here is an example:

```bash
bash eval.sh "data/data_release.json" "llm_generations/glm4.jsonl" "evaluation_results" "<openai_API_KEY>" "<openai_API_KEY>" "glm4"
```

In this example, you can find the results of each scored question in `evaluation_results/glm4_final_results.json`. The statistical data of the final results will be saved in `evaluation_results/glm4_statistics.json`.

## ğŸ‘ Citation

```
@article{wen2024benchmarking,
  title={Benchmarking Complex Instruction-Following with Multiple Constraints Composition},
  author={Wen, Bosi and Ke, Pei and Gu, Xiaotao and Wu, Lindong and Huang, Hao and Zhou, Jinfeng and Li, Wenchuang and Hu, Binxin and Gao, Wendy and Xu, Jiaxin and others},
  journal={arXiv preprint arXiv:2407.03978},
  year={2024}
}
```

Please kindly cite our paper if this paper and the codes are helpful.
